---
layout: archive
title: "Projects and Experiences"
permalink: /projects/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}


* ### Catastrophic Forgetting in Multi-Modal Large Language Models
    * Sep 2023-Present in Tencent Youtu Lab
    * Pioneered the first comprehensive exploration and revelation of catastrophic forgetting in MLLMs such as InstructBLIP and LLaVa. 
    * Addressed the issue through an innovative training-free model grafting technique.

* ### Prompt Tuning in Large Vision-Language Models Based on Neural Collapse
    * April 2023-Aug 2023 in DCD lab, Zhejiang University
    * The first exploration of large vision-language models through the lens of neural collapse in deep learning theory.
    * Tackle class imbalance in generalization tasks for large vision-language models by leveraging neural collapse theory.

* ### Out-of-Distribution Detection based on Self-Attention Mechanism
    * Dec 2022-Mar 2023 in Huawei Noah's Ark Lab
    * Addressed the issue of inconsistent source-target label spaces in Universal Domain Adaptation directly using self-attention in ViT.
    * This research has been accepted at ICCV 2023.

* ### Unifying Domain Adaptation Variants with Label Heterogeneity based on GFlowNet
    * May 2022-Nov 2022 in Huawei Noah's Ark Lab
    * Introduced a comprehensive problem called Generalized Universal Domain Adaptation, achieving a unification of all Domain Adaptation sub-problems involving label heterogeneity.
    * Implemented an exploration-aware active learning strategy based on Generative Flow Networks to effectively address GUDA.
    * This research has been accepted at ACM Multimedia 2023.
