Projects
======

* ### Catastrophic Forgetting in Multi-Modal Large Language Models
    * Tencent Youtu Lab
    * Pioneered the first comprehensive exploration and revelation of catastrophic forgetting in MLLMs such as InstructBLIP and LLaVa. 
    * Addressed the issue through an innovative training-free model grafting technique.

* ### Prompt Tuning in Large Vision-Language Models Based on Neural Collapse
    * The first exploration of large vision-language models through the lens of neural collapse in deep learning theory.
    * Tackle class imbalance in generalization tasks for large vision-language models by leveraging neural collapse theory.

* ### Out-of-Distribution Detection based on Self-Attention Mechanism
    * Huawei Noah's Ark Lab
    * Addressed the issue of inconsistent source-target label spaces in Universal Domain Adaptation directly using self-attention in ViT.
    * This research has been accepted at ICCV 2023.

* ### Unifying Domain Adaptation Variants with Label Heterogeneity based on GFlowNet
    * Huawei Noah's Ark Lab
    * Introduced a comprehensive problem called Generalized Universal Domain Adaptation, achieving a unification of all Domain Adaptation sub-problems involving label heterogeneity.
    * Implemented an exploration-aware active learning strategy based on Generative Flow Networks to effectively address GUDA.
    * This research has been accepted at ACM Multimedia 2023.
